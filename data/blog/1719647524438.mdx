---
title: 中文版斯坦福多智能体AI小镇（一）：安装 Ollama
date: 6/30/2024
tags: [AI]
draft: false
summary: 迈出搭建 AI 小镇的第一步，安装 Ollama
---

### 介绍

之前三金有介绍过两个可视化部署大模型的产品：

*   Lmstudio：[没有网络限制！超简单本地部署 Llama3 的方法](https://mp.weixin.qq.com/s?__biz=MzUyODkwNTg3MA==\&mid=2247484427\&idx=1\&sn=71f8590c6a0147e6974a972e4f0388cb\&chksm=fa6867eacd1feefc7b9cd98f434524d1d84168071f1dcef915a627717d3a935bb88cd0a29fc2\&token=2094085037\&lang=zh_CN#rd)
*   Jan：[18k star 的开源本地部署大模型利器-Jan，支持启动本地服务](https://mp.weixin.qq.com/s?__biz=MzUyODkwNTg3MA==\&mid=2247484607\&idx=1\&sn=7429aeedd2d53b563ad3207d5add86b3\&chksm=fa68675ecd1fee4833470d58daf206160f697191cd6aaedfb56688f8bf269db1b04d7c683e3d\&token=2094085037\&lang=zh_CN#rd)

这两款产品都非常好用，且对于用户来说没有啥难度，哪怕是网络限制，三金也在文章中给出了解决方案，只要电脑配置跟得上即可轻松玩转大模型。

不过这两款产品在 AI 界的受欢迎程度还是远低于**本地部署大模型的鼻祖——Ollama**，Ollama 在 Github 上**拥有 76k 的 star**，star 数一直呈增长的趋势，那为啥三金之前没有介绍过它呢？

这是因为 Ollama 虽然很火，但是对于普通人而言有一定的上手难度，而且并没有直接提供可视化操作的界面，大多数情况下我们需要在终端命令行使用它。（虽然也可以搭配一些工具实现可视化，但是还是不如之前两个直接）

那为什么今天又突然要介绍它呢？是因为我们要使用它来**搭建一个中文版的斯坦福多智能体 AI 小镇**：

![](https://note.ihsxu.com/api/imgs/1719649157434.webp)

> AI 小镇是麻省理工出品的开源项目，它是一个可以构建和定制属于自己的 AI 城镇的项目。有点像我的世界，但是里面的人物都是 AI 生成的。

废话不多说，我们先安装 Ollama，迈出搭建 AI 小镇的第一步！

### 安装

首先，我们访问 Ollama 的[官方](https://ollama.com/)，直接点击 Download 开始下载安装：

![](https://note.ihsxu.com/api/imgs/1719649543517.webp)

下载好之后双击安装包进行安装即可，流程是「Next - Install Command - Finished」：

![](https://note.ihsxu.com/api/imgs/1719649737074.webp)

### 使用

安装好之后，打开终端测试一下是否可以正常运行：

```bash
ollama --version
```

![](https://note.ihsxu.com/api/imgs/1719649888530.webp)

可以看到已经安装成功啦\~

### 测试

因为我们要**使用 Qwen2 7B 来搭建智能体小镇**，所以这里我们以 Qwen2 7B 模型为例进行测试。

```bash
ollama run qwen2:7b
```

ollama 会帮我拉取大模型并开始运行，我们可以直接在终端和大模型进行交互：

![](https://note.ihsxu.com/api/imgs/1719651589824.webp)

### 可视化界面

不过对于非技术出身的小伙伴来说，可能还是更喜欢可视化的操作，这里给大家介绍一下 [open-webui](https://docs.openwebui.com/)，结合它可以实现白屏化交互：

![](https://note.ihsxu.com/api/imgs/1719661295915.webp)

open-webui 提供了 docker 部署的方式，也可以选择手动安装。

如果是**本地已经启动了 Ollama 的小伙伴，尤其是 MacOS 操作系统，推荐使用手动安装的方式**。对于 Linux 操作系统和 Windows 操作系统，则推荐 Docker 优先。

如果没有在本地启动 Ollama，则可以选择使用 docker-compose 一键部署。

详细教程可以阅读官方文档哦～

